{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fusermount: entry for /home/denis/Documents/projects/uncertainty_evaluation/experiments/uncertainty/notebooks/results not found in /etc/mtab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baselines\n",
      "CIFAR10\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p results\n",
    "fusermount -uz results\n",
    "sshfs compute.ies:/mnt/work/dhuseljic/results/uncertainty results\n",
    "ls results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test_stats', 'misc'])\n"
     ]
    }
   ],
   "source": [
    "def load_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_results(path):\n",
    "    path = Path(path)\n",
    "    assert path.is_dir(), 'Path does not exist.'\n",
    "        \n",
    "    results = {}\n",
    "\n",
    "    exp_json = path / 'results_final.json'\n",
    "    exp_cfg = path / '.hydra' / 'config.yaml'\n",
    "    try:\n",
    "        cfg =  OmegaConf.load(exp_cfg)\n",
    "        data = load_json(exp_json)\n",
    "    except:\n",
    "        print(f'{path} has missing results.')\n",
    "        return\n",
    "\n",
    "    results['cfg'] = cfg\n",
    "    # results['checkpoint'] = checkpoint\n",
    "    results['results'] = data\n",
    "    return results\n",
    "\n",
    "def get_experiments(result_path, glob_pattern, train_results=False):\n",
    "    # Aggregate results over multiple glob pattern such as seeds\n",
    "    experiments = []\n",
    "    for exp_path in result_path.glob(glob_pattern):\n",
    "        d = load_results(exp_path)\n",
    "        experiments.append(d)\n",
    "    assert len(experiments) != 0, f'No experiments found for {result_path}.'\n",
    "    return experiments\n",
    "\n",
    "path = f'results/baselines/CIFAR10/resnet18/'\n",
    "exp_results = get_experiments(Path(path), 'seed1')\n",
    "# metrics = get_metric_values(exp_results)\n",
    "print(exp_results[0]['results'].keys())\n",
    "# print(len(exp_results[0]['results']['train_history']))\n",
    "# print(exp_results[0]['results']['test_stats'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 134.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Baseline results\n",
    "dataset = 'CIFAR10'\n",
    "experiments = {}\n",
    "experiments.update({\n",
    "    'standard': f'results/baselines/CIFAR10/resnet18/',\n",
    "    'label_smoothing': f'results/baselines/CIFAR10/resnet18_labelsmoothing/',\n",
    "    # 'mixup': f'results/baselines/CIFAR10/resnet18_mixup/',\n",
    "    'sngp': f'results/baselines/CIFAR10/resnet18_sngp/',\n",
    "})\n",
    "\n",
    "\n",
    "all_results = {}\n",
    "for exp_name, exp_path in tqdm(experiments.items()):\n",
    "    all_results[exp_name] = get_experiments(Path(exp_path), 'seed1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                 |   accuracy |     brier |       ece |       ace |   auroc_SVHN |   auroc_CIFAR100 |\n",
      "|:----------------|-----------:|----------:|----------:|----------:|-------------:|-----------------:|\n",
      "| standard        |     0.9489 | 0.082059  | 0.0274236 | 0.036072  |     0.885258 |         0.874181 |\n",
      "| label_smoothing |     0.9503 | 0.0793098 | 0.0402002 | 0.0340034 |     0.817936 |         0.814172 |\n",
      "| sngp            |     0.8614 | 0.200464  | 0.0214757 | 0.0247449 |     0.882446 |         0.822672 |\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for exp_name in experiments:\n",
    "    d = {}\n",
    "    for metric_key in ['accuracy', 'brier', 'ece', 'ace', 'auroc_SVHN', 'auroc_CIFAR100']: \n",
    "        vals = [seed_results['results']['test_stats'][metric_key] for seed_results in all_results[exp_name]]\n",
    "        d[metric_key] = np.mean(vals)\n",
    "    result_dict[exp_name] = d\n",
    "print(pd.DataFrame(result_dict).T.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertainty_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfeabd65c26caef9e4834d0951a792d9f59ccda2d72b0cad1a1a6596669e4d0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
