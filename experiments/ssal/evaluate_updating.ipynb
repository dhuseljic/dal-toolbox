{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 experiments\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "client = mlflow.tracking.MlflowClient(tracking_uri='file:///home/dhuseljic/mlflow/ssal/updating')\n",
    "experiment_id = '473961987545598420'\n",
    "runs = client.search_runs(experiment_ids=experiment_id)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 experiments\n"
     ]
    }
   ],
   "source": [
    "query_list = [\n",
    "    \"parameter.dataset_name = 'cifar10'\",\n",
    "    \"parameter.model.name = 'laplace'\",\n",
    "    \"parameter.dino_model_name = 'dinov2_vitl14'\",\n",
    "    \"parameter.num_init_samples = '50'\",\n",
    "    \"parameter.num_new_samples = '10'\",\n",
    "    \"parameter.model.num_epochs = '500'\",\n",
    "    \"parameter.update_lmb = '1'\",\n",
    "    \"parameter.update_gamma = '1'\",\n",
    "]\n",
    "query = ' and '.join(query_list)\n",
    "runs = client.search_runs(experiment_ids=experiment_id, filter_string=query)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8995000123977661 0.8665000200271606 0.9003999829292297\n",
      "0.9366000294685364 0.9365000128746033 0.935699999332428\n",
      "0.8824999928474426 0.8824999928474426 0.8822000026702881\n",
      "0.8693000078201294 0.8691999912261963 0.8682000041007996\n",
      "0.9028000235557556 0.34470000863075256 0.9168000221252441\n",
      "Improvment 195 / 200\n"
     ]
    }
   ],
   "source": [
    "base_accuracies = []\n",
    "updated_accuracies = []\n",
    "retrained_accuracies = []\n",
    "\n",
    "base_ACEs = []\n",
    "updated_ACEs = []\n",
    "retrained_ACEs = []\n",
    "\n",
    "base_AUPRs = []\n",
    "updated_AUPRs = []\n",
    "retrained_AUPRs = []\n",
    "\n",
    "num_improved = 0\n",
    "for run in runs:\n",
    "    if len(run.data.metrics) == 0: continue # failed run\n",
    "    base_accuracies.append(run.data.metrics['base_accuracy'])\n",
    "    updated_accuracies.append(run.data.metrics['updated_accuracy'])\n",
    "    retrained_accuracies.append(run.data.metrics['retrained_accuracy'])\n",
    "    \n",
    "    base_ACEs.append(run.data.metrics['base_ACE'])\n",
    "    updated_ACEs.append(run.data.metrics['updated_ACE'])\n",
    "    retrained_ACEs.append(run.data.metrics['retrained_ACE'])\n",
    "\n",
    "    base_AUPRs.append(run.data.metrics['base_AUPR'])\n",
    "    updated_AUPRs.append(run.data.metrics['updated_AUPR'])\n",
    "    retrained_AUPRs.append(run.data.metrics['retrained_AUPR'])\n",
    "\n",
    "    if updated_accuracies[-1] > base_accuracies[-1]:\n",
    "        num_improved += 1\n",
    "    else:\n",
    "        print(base_accuracies[-1], updated_accuracies[-1], retrained_accuracies[-1])\n",
    "        # Retraining test?\n",
    "\n",
    "    # if np.isnan(updated_ACEs[-1]):\n",
    "    #    print(run.data.params['random_seed'], base_accuracies[-1], updated_accuracies[-1])\n",
    "print('Improvment {} / {}'.format(num_improved, len(runs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & base & updated & retrained \\\\\n",
      "\\midrule\n",
      "accuracy & 0.660 & 0.677 & 0.710 \\\\\n",
      "ACE & 0.134 & 0.136 & 0.139 \\\\\n",
      "AUPR & 0.657 & 0.666 & 0.689 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'accuracy': [np.mean(base_accuracies), np.mean(updated_accuracies), np.mean(retrained_accuracies)],\n",
    "    'ACE': [np.mean(base_ACEs), np.mean(updated_ACEs), np.mean(retrained_ACEs)],\n",
    "    'AUPR': [np.mean(base_AUPRs), np.mean(updated_AUPRs), np.mean(retrained_AUPRs)],\n",
    "}\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "df.columns = ['base', 'updated', 'retrained']\n",
    "\n",
    "print(df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 experiments\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "client = mlflow.tracking.MlflowClient(tracking_uri='file:///home/dhuseljic/mlflow/ssal/updating_svhn')\n",
    "experiment_id = '620601089375835831'\n",
    "\n",
    "runs = client.search_runs(experiment_ids=experiment_id)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 experiments\n"
     ]
    }
   ],
   "source": [
    "query_list = [\n",
    "    \"parameter.model.name = 'laplace'\",\n",
    "    \"parameter.dino_model_name = 'dinov2_vitl14'\",\n",
    "    \"parameter.num_init_samples = '500'\",\n",
    "    \"parameter.num_new_samples = '100'\",\n",
    "    \"parameter.model.num_epochs = '500'\",\n",
    "    \"parameter.update_lmb = '1'\",\n",
    "    \"parameter.update_gamma = '1'\",\n",
    "]\n",
    "query = ' and '.join(query_list)\n",
    "runs = client.search_runs(experiment_ids=experiment_id, filter_string=query)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_accuracies = []\n",
    "updated_accuracies = []\n",
    "retrained_accuracies = []\n",
    "\n",
    "base_ACEs = []\n",
    "updated_ACEs = []\n",
    "retrained_ACEs = []\n",
    "\n",
    "base_AUPRs = []\n",
    "updated_AUPRs = []\n",
    "retrained_AUPRs = []\n",
    "\n",
    "for run in runs:\n",
    "    if len(run.data.metrics) == 0: continue # failed run\n",
    "    base_accuracies.append(run.data.metrics['base_accuracy'])\n",
    "    updated_accuracies.append(run.data.metrics['updated_accuracy'])\n",
    "    retrained_accuracies.append(run.data.metrics['retrained_accuracy'])\n",
    "    \n",
    "    base_ACEs.append(run.data.metrics['base_ACE'])\n",
    "    updated_ACEs.append(run.data.metrics['updated_ACE'])\n",
    "    retrained_ACEs.append(run.data.metrics['retrained_ACE'])\n",
    "\n",
    "    base_AUPRs.append(run.data.metrics['base_AUPR'])\n",
    "    updated_AUPRs.append(run.data.metrics['updated_AUPR'])\n",
    "    retrained_AUPRs.append(run.data.metrics['retrained_AUPR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & base & updated & retrained \\\\\n",
      "\\midrule\n",
      "accuracy & 0.334 & 0.180 & 0.346 \\\\\n",
      "ACE & 0.061 & 0.038 & 0.062 \\\\\n",
      "AUPR & 0.953 & 0.412 & 0.950 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'accuracy': [np.mean(base_accuracies), np.mean(updated_accuracies), np.mean(retrained_accuracies)],\n",
    "    'ACE': [np.mean(base_ACEs), np.mean(updated_ACEs), np.mean(retrained_ACEs)],\n",
    "    'AUPR': [np.mean(base_AUPRs), np.mean(updated_AUPRs), np.mean(retrained_AUPRs)],\n",
    "}\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "df.columns = ['base', 'updated', 'retrained']\n",
    "\n",
    "print(df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399 experiments\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "client = mlflow.tracking.MlflowClient(tracking_uri='file:///home/dhuseljic/mlflow/ssal/updating_cifar100')\n",
    "experiment_id = '580146024816781076'\n",
    "\n",
    "runs = client.search_runs(experiment_ids=experiment_id)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 experiments\n"
     ]
    }
   ],
   "source": [
    "query_list = [\n",
    "    \"parameter.model.name = 'laplace'\",\n",
    "    \"parameter.dino_model_name = 'dinov2_vitl14'\",\n",
    "    \"parameter.num_init_samples = '500'\",\n",
    "    \"parameter.num_new_samples = '100'\",\n",
    "    \"parameter.model.num_epochs = '500'\",\n",
    "    \"parameter.update_lmb = '1'\",\n",
    "    \"parameter.update_gamma = '1'\",\n",
    "]\n",
    "query = ' and '.join(query_list)\n",
    "runs = client.search_runs(experiment_ids=experiment_id, filter_string=query)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvment 96 / 200\n"
     ]
    }
   ],
   "source": [
    "base_accuracies = []\n",
    "updated_accuracies = []\n",
    "retrained_accuracies = []\n",
    "\n",
    "base_ACEs = []\n",
    "updated_ACEs = []\n",
    "retrained_ACEs = []\n",
    "\n",
    "base_AUPRs = []\n",
    "updated_AUPRs = []\n",
    "retrained_AUPRs = []\n",
    "\n",
    "num_improved = 0\n",
    "for run in runs:\n",
    "    if len(run.data.metrics) == 0: continue # failed run\n",
    "    base_accuracies.append(run.data.metrics['base_accuracy'])\n",
    "    updated_accuracies.append(run.data.metrics['updated_accuracy'])\n",
    "    retrained_accuracies.append(run.data.metrics['retrained_accuracy'])\n",
    "    \n",
    "    base_ACEs.append(run.data.metrics['base_ACE'])\n",
    "    updated_ACEs.append(run.data.metrics['updated_ACE'])\n",
    "    retrained_ACEs.append(run.data.metrics['retrained_ACE'])\n",
    "\n",
    "    base_AUPRs.append(run.data.metrics['base_AUPR'])\n",
    "    updated_AUPRs.append(run.data.metrics['updated_AUPR'])\n",
    "    retrained_AUPRs.append(run.data.metrics['retrained_AUPR'])\n",
    "\n",
    "    if updated_accuracies[-1] > base_accuracies[-1]:\n",
    "        num_improved += 1\n",
    "\n",
    "    # if np.isnan(updated_ACEs[-1]):\n",
    "    #    print(run.data.params['random_seed'], base_accuracies[-1], updated_accuracies[-1])\n",
    "print('Improvment {} / {}'.format(num_improved, len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & base & updated & retrained \\\\\n",
      "\\midrule\n",
      "accuracy & 0.737 & 0.559 & 0.759 \\\\\n",
      "ACE & 0.028 & NaN & 0.029 \\\\\n",
      "AUPR & 0.834 & 0.749 & 0.848 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'accuracy': [np.mean(base_accuracies), np.mean(updated_accuracies), np.mean(retrained_accuracies)],\n",
    "    'ACE': [np.mean(base_ACEs), np.mean(updated_ACEs), np.mean(retrained_ACEs)],\n",
    "    'AUPR': [np.mean(base_AUPRs), np.mean(updated_AUPRs), np.mean(retrained_AUPRs)],\n",
    "}\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "df.columns = ['base', 'updated', 'retrained']\n",
    "\n",
    "print(df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 598 experiments\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "client = mlflow.tracking.MlflowClient(tracking_uri='file:///home/dhuseljic/mlflow/ssal/updating_food101')\n",
    "experiment_id = '102008380090652354'\n",
    "\n",
    "runs = client.search_runs(experiment_ids=experiment_id)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 experiments\n"
     ]
    }
   ],
   "source": [
    "query_list = [\n",
    "    \"parameter.model.name = 'laplace'\",\n",
    "    \"parameter.dino_model_name = 'dinov2_vitl14'\",\n",
    "    \"parameter.num_init_samples = '1000'\",\n",
    "    \"parameter.num_new_samples = '100'\",\n",
    "    \"parameter.model.num_epochs = '500'\",\n",
    "    \"parameter.update_lmb = '1'\",\n",
    "    \"parameter.update_gamma = '1'\",\n",
    "]\n",
    "query = ' and '.join(query_list)\n",
    "runs = client.search_runs(experiment_ids=experiment_id, filter_string=query)\n",
    "print('Found {} experiments'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvment 59 / 100\n"
     ]
    }
   ],
   "source": [
    "base_accuracies = []\n",
    "updated_accuracies = []\n",
    "retrained_accuracies = []\n",
    "\n",
    "base_ACEs = []\n",
    "updated_ACEs = []\n",
    "retrained_ACEs = []\n",
    "\n",
    "base_AUPRs = []\n",
    "updated_AUPRs = []\n",
    "retrained_AUPRs = []\n",
    "\n",
    "num_improved = 0\n",
    "for run in runs:\n",
    "    if len(run.data.metrics) == 0: continue # failed run\n",
    "    base_accuracies.append(run.data.metrics['base_accuracy'])\n",
    "    updated_accuracies.append(run.data.metrics['updated_accuracy'])\n",
    "    retrained_accuracies.append(run.data.metrics['retrained_accuracy'])\n",
    "    \n",
    "    base_ACEs.append(run.data.metrics['base_ACE'])\n",
    "    updated_ACEs.append(run.data.metrics['updated_ACE'])\n",
    "    retrained_ACEs.append(run.data.metrics['retrained_ACE'])\n",
    "\n",
    "    base_AUPRs.append(run.data.metrics['base_AUPR'])\n",
    "    updated_AUPRs.append(run.data.metrics['updated_AUPR'])\n",
    "    retrained_AUPRs.append(run.data.metrics['retrained_AUPR'])\n",
    "\n",
    "    if updated_accuracies[-1] > base_accuracies[-1]:\n",
    "        num_improved += 1\n",
    "\n",
    "    # if np.isnan(updated_ACEs[-1]):\n",
    "    #    print(run.data.params['random_seed'], base_accuracies[-1], updated_accuracies[-1])\n",
    "print('Improvment {} / {}'.format(num_improved, len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      " & base & updated & retrained \\\\\n",
      "\\midrule\n",
      "accuracy & 0.821 & 0.821 & 0.833 \\\\\n",
      "ACE & 0.037 & 0.037 & 0.039 \\\\\n",
      "AUPR & 1.000 & 1.000 & 1.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'accuracy': [np.mean(base_accuracies), np.mean(updated_accuracies), np.mean(retrained_accuracies)],\n",
    "    'ACE': [np.mean(base_ACEs), np.mean(updated_ACEs), np.mean(retrained_ACEs)],\n",
    "    'AUPR': [np.mean(base_AUPRs), np.mean(updated_AUPRs), np.mean(retrained_AUPRs)],\n",
    "}\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "df.columns = ['base', 'updated', 'retrained']\n",
    "\n",
    "print(df.to_latex(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 experiments.\n"
     ]
    }
   ],
   "source": [
    "query_list = [\n",
    "    \"parameter.dino_model_name = 'dinov2_vitl14'\",\n",
    "    \"parameter.num_init_samples = '10'\",\n",
    "    \"parameter.num_new_samples = '10'\",\n",
    "    \"parameter.model.num_epochs = '500'\",\n",
    "]\n",
    "query = ' and '.join(query_list)\n",
    "runs = client.search_runs(experiment_ids=experiment_id, filter_string=query)\n",
    "print('Found {} experiments.'.format(len(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "updating_times_standard = []\n",
    "retraining_times_standard = []\n",
    "\n",
    "updating_times_optimal = []\n",
    "retraining_times_optimal = []\n",
    "\n",
    "updating_times_lr = []\n",
    "retraining_times_lr = []\n",
    "\n",
    "for run in runs:\n",
    "    if len(run.data.metrics) == 0: continue\n",
    "    if run.data.params['update_lmb'] == '1' and run.data.params['update_gamma'] == '1':\n",
    "        updating_times_standard.append(run.data.metrics['updated_updating_time'])\n",
    "        retraining_times_standard.append(run.data.metrics['retrained_retraining_time'])\n",
    "    elif run.data.params['update_lmb'] == '10' and run.data.params['update_gamma'] == '1':\n",
    "        updating_times_optimal.append(run.data.metrics['updated_updating_time'])\n",
    "        retraining_times_optimal.append(run.data.metrics['retrained_retraining_time'])\n",
    "    elif run.data.params['update_lmb'] == '1' and run.data.params['update_gamma'] == '10':\n",
    "        updating_times_lr.append(run.data.metrics['updated_updating_time'])\n",
    "        retraining_times_lr.append(run.data.metrics['retrained_retraining_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|          |   base |   updated |   retrained |\n",
      "|:---------|-------:|----------:|------------:|\n",
      "| standard |      0 | 0.0703375 |     3.66768 |\n",
      "| optimal  |      0 | 0.589755  |     3.47345 |\n",
      "| lr       |      0 | 0.0709146 |     3.81529 |\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'standard': [0, np.mean(updating_times_standard), np.mean(retraining_times_standard)],\n",
    "    'optimal': [0, np.mean(updating_times_optimal), np.mean(retraining_times_optimal)],\n",
    "    'lr': [0, np.mean(updating_times_lr), np.mean(retraining_times_lr)],\n",
    "}\n",
    "df = pd.DataFrame(results).T\n",
    "df.columns = ['base', 'updated', 'retrained']\n",
    "\n",
    "print(df.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 experiments.\n",
      "ACC: 0.936\n",
      "ACE: 0.038\n",
      "AUROC: 0.948\n",
      "AUPR: 0.935\n"
     ]
    }
   ],
   "source": [
    "query_list = [\n",
    "    \"parameter.model.name = 'deterministic'\",\n",
    "    \"parameter.dino_model_name = 'dinov2_vitl14'\",\n",
    "    \"parameter.num_init_samples = '100'\",\n",
    "    \"parameter.num_new_samples = '10'\",\n",
    "    \"parameter.model.num_epochs = '500'\",\n",
    "    \"parameter.update_lmb = '1'\",\n",
    "    \"parameter.update_gamma = '1'\",\n",
    "]\n",
    "query = ' and '.join(query_list)\n",
    "runs = client.search_runs(experiment_ids=experiment_id, filter_string=query)\n",
    "print('Found {} experiments.'.format(len(runs)))\n",
    "print('ACC: {:.3f}'.format(np.mean([run.data.metrics['base_accuracy'] for run in runs])))\n",
    "print('ACE: {:.3f}'.format(np.mean([run.data.metrics['base_ACE'] for run in runs])))\n",
    "print('AUROC: {:.3f}'.format(np.mean([run.data.metrics['base_AUROC'] for run in runs])))\n",
    "print('AUPR: {:.3f}'.format(np.mean([run.data.metrics['base_AUPR'] for run in runs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dal-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
