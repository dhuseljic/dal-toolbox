{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dal_toolbox import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading data set..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/home/lukas/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 633.34it/s]\n",
      "Loading cached processed dataset at /home/lukas/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-51be2db130f4b945_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Apply Tokenization on the complete dataset..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/lukas/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548/cache-7d7713f7f40f62a8_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "data = datasets.agnews.AGNews(\n",
    "    modelname=\"bert-base-cased\",\n",
    "    dataset_path=\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/home/lukas/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "                                                                     \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#len(data.train_dataset)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39;49mtest_dataset)\n",
      "File \u001b[0;32m~/projects/uncertainty-evaluation/dal_toolbox/datasets/activeglae/agnews.py:47\u001b[0m, in \u001b[0;36mAGNews.test_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m test_ds \u001b[39m=\u001b[39m load_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_path, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m test_ds \u001b[39m=\u001b[39m test_ds\u001b[39m.\u001b[39mmap(\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, num_proc\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m test_ds \u001b[39m=\u001b[39m test_ds\u001b[39m.\u001b[39mremove_columns(\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(test_ds\u001b[39m.\u001b[39mcolumn_names)\u001b[39m-\u001b[39m\u001b[39mset\u001b[39m([\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     48\u001b[0m )         \n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m test_ds\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/datasets/arrow_dataset.py:2778\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2778\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/datasets/arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2760\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2761\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2762\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2763\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2764\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2765\u001b[0m )\n\u001b[1;32m   2766\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/datasets/formatting/formatting.py:575\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    573\u001b[0m     _raise_bad_key_type(key)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     _check_valid_column_key(key, table\u001b[39m.\u001b[39;49mcolumn_names)\n\u001b[1;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    577\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/datasets/formatting/formatting.py:515\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_column_key\u001b[39m(key: \u001b[39mstr\u001b[39m, columns: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns:\n\u001b[0;32m--> 515\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask']\""
     ]
    }
   ],
   "source": [
    "#len(data.train_dataset)\n",
    "len(data.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /tmp/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4784128/170498071 [00:08<05:04, 544535.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mCIFAR10(\u001b[39m'\u001b[39;49m\u001b[39m/tmp\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/uncertainty-evaluation/dal_toolbox/datasets/cifar.py:21\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, dataset_path, mean, std, val_split, seed)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean \u001b[39m=\u001b[39m mean\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstd \u001b[39m=\u001b[39m std\n\u001b[0;32m---> 21\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(dataset_path, val_split, seed)\n",
      "File \u001b[0;32m~/projects/uncertainty-evaluation/dal_toolbox/datasets/base.py:16\u001b[0m, in \u001b[0;36mAbstractData.__init__\u001b[0;34m(self, dataset_path, val_split, seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrng \u001b[39m=\u001b[39m setup_rng(seed)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Download and get dataset\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_datasets()\n\u001b[1;32m     18\u001b[0m \u001b[39m# Train Validation split on training dataset\u001b[39;00m\n\u001b[1;32m     19\u001b[0m train_indices, val_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_train_val_indices(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_train_dataset))\n",
      "File \u001b[0;32m~/projects/uncertainty-evaluation/dal_toolbox/datasets/base.py:42\u001b[0m, in \u001b[0;36mAbstractData._download_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m     dist\u001b[39m.\u001b[39mbarrier()  \u001b[39m# Make sure that only the process with rank 0 downloads the data\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_datasets()\n",
      "File \u001b[0;32m~/projects/uncertainty-evaluation/dal_toolbox/datasets/cifar.py:28\u001b[0m, in \u001b[0;36mCIFAR10.download_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_datasets\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     datasets\u001b[39m.\u001b[39;49mCIFAR10(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_path, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     29\u001b[0m     datasets\u001b[39m.\u001b[39mCIFAR10(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_path, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/torchvision/datasets/cifar.py:65\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m train  \u001b[39m# training set or test set\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload()\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_integrity():\n\u001b[1;32m     68\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/torchvision/datasets/cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFiles already downloaded and verified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m download_and_extract_archive(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot, filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, md5\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtgz_md5)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/torchvision/datasets/utils.py:434\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename:\n\u001b[1;32m    432\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(url)\n\u001b[0;32m--> 434\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[1;32m    436\u001b[0m archive \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(download_root, filename)\n\u001b[1;32m    437\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting \u001b[39m\u001b[39m{\u001b[39;00marchive\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mextract_root\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/torchvision/datasets/utils.py:144\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m fpath)\n\u001b[0;32m--> 144\u001b[0m     _urlretrieve(url, fpath)\n\u001b[1;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m url[:\u001b[39m5\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/torchvision/datasets/utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_urlretrieve\u001b[39m(url: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, chunk_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(url, headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: USER_AGENT})) \u001b[39mas\u001b[39;00m response:\n\u001b[0;32m---> 48\u001b[0m         _save_response_content(\u001b[39miter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m: response\u001b[39m.\u001b[39;49mread(chunk_size), \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m), filename, length\u001b[39m=\u001b[39;49mresponse\u001b[39m.\u001b[39;49mlength)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/torchvision/datasets/utils.py:37\u001b[0m, in \u001b[0;36m_save_response_content\u001b[0;34m(content, destination, length)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_response_content\u001b[39m(\n\u001b[1;32m     32\u001b[0m     content: Iterator[\u001b[39mbytes\u001b[39m],\n\u001b[1;32m     33\u001b[0m     destination: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     34\u001b[0m     length: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(destination, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fh, tqdm(total\u001b[39m=\u001b[39mlength) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m---> 37\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m content:\n\u001b[1;32m     38\u001b[0m             \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m     39\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunk:\n\u001b[1;32m     40\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/site-packages/torchvision/datasets/utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_urlretrieve\u001b[39m(url: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, chunk_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(url, headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: USER_AGENT})) \u001b[39mas\u001b[39;00m response:\n\u001b[0;32m---> 48\u001b[0m         _save_response_content(\u001b[39miter\u001b[39m(\u001b[39mlambda\u001b[39;00m: response\u001b[39m.\u001b[39;49mread(chunk_size), \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m), filename, length\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mlength)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/dal-toolbox/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = datasets.CIFAR10('/tmp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dal-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
