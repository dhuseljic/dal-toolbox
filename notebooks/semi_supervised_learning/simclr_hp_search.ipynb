{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import warnings\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn import datasets\n",
    "from dal_toolbox.utils import seed_everything\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_contour(model, X_l, y_l, X_u, y_u, ax=None, feature_extractor=None):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    origin = 'lower'\n",
    "    if ax:\n",
    "        plt.sca(ax)\n",
    "    domain = 3\n",
    "    xx, yy = torch.meshgrid(torch.linspace(-domain, domain, 51), torch.linspace(-domain, domain, 51), indexing='ij')\n",
    "    zz = torch.stack((xx.flatten(), yy.flatten()), dim=1)\n",
    "\n",
    "    if feature_extractor:\n",
    "        feature_extractor.eval()\n",
    "        feature_extractor.cpu()\n",
    "        zz = feature_extractor(zz)\n",
    "    logits = model(zz)\n",
    "    probas = logits.softmax(-1)\n",
    "    zz = probas[:, 1].view(xx.shape)\n",
    "\n",
    "    plt.scatter(X_l[:, 0], X_l[:, 1], c=y_l, s=50, edgecolors='red')\n",
    "    plt.scatter(X_u[:, 0], X_u[:, 1], c=y_u, s=15, alpha=.25)\n",
    "    CS = plt.contourf(xx, yy, zz, alpha=.8, zorder=-1, levels=np.linspace(0, 1, 6), origin=origin)\n",
    "    CS2 = plt.contour(CS, levels=[0.5], colors='black', origin=origin)\n",
    "    cbar = plt.colorbar(CS)\n",
    "    cbar.add_lines(CS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochasitc_augmentation(X: torch.tensor, noise: float = 1):\n",
    "    return X + torch.randn_like(X)*noise\n",
    "\n",
    "class SSLDataset(torch.utils.data.TensorDataset):\n",
    "    def __init__(self, data, targets, noise):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.noise = noise\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.data[idx], self.targets[idx]\n",
    "        return [stochasitc_augmentation(X, self.noise), stochasitc_augmentation(X, self.noise)], y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes: int = 2, feature_dim: int = 128, projection_dim: int = 16):\n",
    "        super(Net, self).__init__()\n",
    "        self.base_encoder = nn.Sequential(nn.Linear(2, feature_dim), nn.ReLU())\n",
    "        self.projector = nn.Sequential(nn.Linear(feature_dim, projection_dim), nn.ReLU())\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x, get_projections=False):\n",
    "        x = self.base_encoder(x)\n",
    "        if get_projections:\n",
    "            return self.projector(x)\n",
    "        else:\n",
    "            return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_NUM = 1e9\n",
    "\n",
    "class NTXent(nn.Module):\n",
    "    def __init__(self, batch_size, hidden_norm: bool = True, temperature: int = 1.0, n_views: int = 2):\n",
    "        super(NTXent, self).__init__()\n",
    "        self.sim = nn.CosineSimilarity(dim=-1)\n",
    "        self.temperature = temperature\n",
    "        self.hidden_norm = hidden_norm\n",
    "        self.device='cuda'\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_views = n_views\n",
    "\n",
    "    def forward(self, features):\n",
    "        labels = torch.cat([torch.arange(self.batch_size) for i in range(self.n_views)], dim=0)\n",
    "        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "\n",
    "        similarity_matrix = torch.matmul(features, features.T)\n",
    "\n",
    "        # discard the main diagonal from both: labels and similarities matrix\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(self.device)\n",
    "        labels = labels[~mask].view(labels.shape[0], -1)\n",
    "        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
    "        assert similarity_matrix.shape == labels.shape\n",
    "\n",
    "        # select and combine multiple positives\n",
    "        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
    "\n",
    "        # select only the negatives the negatives\n",
    "        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
    "\n",
    "        logits = torch.cat([positives, negatives], dim=1)\n",
    "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(self.device)\n",
    "\n",
    "        logits = logits / self.temperature\n",
    "        loss = self.ce_loss(logits, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_one_epoch(model, optimizer, criterion, unsupervised_loader, device='cuda'):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    total_loss, n_samples = 0, 0\n",
    "\n",
    "    # Train the epoch\n",
    "    for data, _ in unsupervised_loader:\n",
    "        data = torch.cat(data, dim=0)\n",
    "        data = data.to(device)\n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        # Unsupervised loss\n",
    "        out = model(data, get_projections=True)\n",
    "        loss = criterion(out)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() / batch_size\n",
    "\n",
    "        n_samples += batch_size\n",
    "\n",
    "    return {'loss':total_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, criterion, supervised_loader, device='cuda'):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    total_loss, n_samples, n_correct = 0, 0, 0 \n",
    "\n",
    "    # Train the epoch\n",
    "    for (x, y) in supervised_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "        n_correct += torch.sum(torch.max(out.softmax(-1), dim=-1)[1] == y).item()\n",
    "\n",
    "    return {\n",
    "        'loss':total_loss/n_samples, \n",
    "        'acc':n_correct/n_samples\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    total_loss, n_samples, n_correct = 0, 0, 0 \n",
    "\n",
    "    # Train the epoch\n",
    "    for (x, y) in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "        n_correct += torch.sum(torch.max(out.softmax(-1), dim=-1)[1] == y).item()\n",
    "\n",
    "    return {\n",
    "        'loss':total_loss/n_samples, \n",
    "        'acc':n_correct/n_samples\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(args, seed=42):\n",
    "    X, y = datasets.make_moons(args['n_samples'], noise=.1, random_state=seed)\n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "\n",
    "    random.seed(seed)\n",
    "    test_indices = random.sample(range(X.shape[0]), k =args['n_test_samples'])\n",
    "    train_indices = [i for i in range(X.shape[0]) if i not in test_indices]\n",
    "    labeled_indices = random.sample(train_indices, k=args['n_labeled_samples'])\n",
    "    unlabeled_indices = [i for i in train_indices if i not in labeled_indices]\n",
    "    labeled_ds = torch.utils.data.TensorDataset(X[labeled_indices], y[labeled_indices])\n",
    "    unlabeled_ds = SSLDataset(X[train_indices], y[train_indices], noise=args['noise'])\n",
    "    test_ds = torch.utils.data.TensorDataset(X[test_indices], y[test_indices])\n",
    "\n",
    "    ds_info = {\n",
    "        'train_indices':train_indices,\n",
    "        'test_indices':test_indices,\n",
    "        'labeled_indices':labeled_indices,\n",
    "        'unlabeled_indices':unlabeled_indices,\n",
    "        'n_classes':2,\n",
    "        'X':X,\n",
    "        'y':y\n",
    "    }\n",
    "\n",
    "    return labeled_ds, unlabeled_ds, test_ds, ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pretrain_search_space(args):\n",
    "    points_to_evaluate = None\n",
    "    search_space = {\n",
    "        \"lr\": tune.uniform(1e-3, .5),\n",
    "        \"weight_decay\": tune.uniform(0, .5),\n",
    "        \"noise\": tune.uniform(0, .1)\n",
    "    }\n",
    "    return search_space, points_to_evaluate\n",
    "\n",
    "def build_supervised_search_space(args):\n",
    "    points_to_evaluate = None\n",
    "    search_space = {\n",
    "        \"lr\": tune.uniform(1e-3, .5),\n",
    "        \"weight_decay\": tune.uniform(0, .5),\n",
    "        \"n_epochs\": tune.choice(range(200))\n",
    "    }\n",
    "    return search_space, points_to_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(config, args):\n",
    "    args['pretrain_learning_rate'] = float(config['lr'])\n",
    "    args['pretrain_weight_decay'] = float(config['weight_decay'])\n",
    "    args['noise'] = float(config['noise'])\n",
    "\n",
    "    _, unlabeled_ds, _, _ = build_datasets(args)\n",
    "\n",
    "    #seed_everything(args['random_seed'])\n",
    "    model = Net(feature_dim=args['feature_dim'], projection_dim=args['projection_dim'])\n",
    "    pretrain_optimizer = torch.optim.SGD(model.parameters(), lr=args['pretrain_learning_rate'], momentum=0.9, weight_decay=args['pretrain_weight_decay'], nesterov=True)\n",
    "    pretrain_criterion = NTXent(batch_size=args['pretrain_batch_size'], temperature=args['temperature'], hidden_norm=True)\n",
    "    pretrain_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(pretrain_optimizer, T_max=args['pretrain_n_epochs']-args['n_epochs_warmup'])\n",
    "    pretrain_loader = torch.utils.data.DataLoader(unlabeled_ds, batch_size=args['pretrain_batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "    for i in range(args['pretrain_n_epochs']):\n",
    "        pretrain_stats = pretrain_one_epoch(model, pretrain_optimizer, pretrain_criterion, pretrain_loader)\n",
    "        if i > args['n_epochs_warmup']:\n",
    "            pretrain_lr_scheduler.step()\n",
    "\n",
    "    return {'unsup_loss':pretrain_stats['loss']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, args, labeled_ds, val_ds, model, pretrained_model_state):\n",
    "    args['supervised_learning_rate'] = float(config['lr'])\n",
    "    args['supervised_weight_decay'] = float(config['weight_decay'])\n",
    "    args['n_epochs_supervised'] = float(config['n_epochs'])\n",
    "\n",
    "    model.load_state_dict(pretrained_model_state)\n",
    "\n",
    "    # freeze all layers but the last fc\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    supervised_optimizer = torch.optim.SGD(model.parameters(), lr=args['supervised_learning_rate'], momentum=0.9, weight_decay=args['supervised_weight_decay'], nesterov=True)\n",
    "    supervised_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(supervised_optimizer, T_max=args['supervised_n_epochs'])\n",
    "    supervised_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(labeled_ds, batch_size=args['supervised_batch_size'], shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "    for j in range(args['supervised_n_epochs']):\n",
    "        train_one_epoch(model, supervised_optimizer, supervised_criterion, train_loader)\n",
    "        supervised_lr_scheduler.step()\n",
    "\n",
    "    test_stats = evaluate(model, supervised_criterion, val_loader)\n",
    "\n",
    "    return {'val_acc':test_stats['acc']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'n_samples':400,\n",
    "    'n_test_samples':100,\n",
    "    'n_labeled_samples':8,\n",
    "\n",
    "    'feature_dim':128,\n",
    "    'projection_dim':64,\n",
    "    'n_classes':2,\n",
    "\n",
    "    'pretrain_batch_size':4,\n",
    "    'pretrain_learning_rate':1e-1,\n",
    "    'pretrain_weight_decay':5e-4,\n",
    "    'n_epochs_warmup':10,\n",
    "    'pretrain_n_epochs':500,\n",
    "    'noise':0.05,\n",
    "    'temperature':1,\n",
    "\n",
    "    'supervised_n_epochs':100,\n",
    "    'supervised_batch_size':64,\n",
    "    'supervised_learning_rate':1e-1,\n",
    "    'supervised_weight_decay':5e-4,\n",
    "    \n",
    "    'random_seed':42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 09:25:31,207\tINFO worker.py:1616 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[32m[I 2023-04-27 09:25:31,586]\u001b[0m A new study created in memory with name: optuna\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-27 09:25:58</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:26.74        </td></tr>\n",
       "<tr><td>Memory:      </td><td>24.9/31.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/1 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name       </th><th>status  </th><th>loc                 </th><th style=\"text-align: right;\">      lr</th><th style=\"text-align: right;\">     noise</th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>pretrain_41da4ef9</td><td>RUNNING </td><td>141.51.131.177:12651</td><td style=\"text-align: right;\">0.166634</td><td style=\"text-align: right;\">0.00634978</td><td style=\"text-align: right;\">     0.0858252</td></tr>\n",
       "<tr><td>pretrain_693f564d</td><td>PENDING </td><td>                    </td><td style=\"text-align: right;\">0.459174</td><td style=\"text-align: right;\">0.0295309 </td><td style=\"text-align: right;\">     0.360224 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 09:25:57,341\tWARNING tune.py:184 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-04-27 09:25:58,347\tERROR tune.py:941 -- Trials did not complete: [pretrain_41da4ef9, pretrain_693f564d]\n",
      "2023-04-27 09:25:58,347\tINFO tune.py:945 -- Total run time: 26.77 seconds (26.74 seconds for the tuning loop).\n",
      "2023-04-27 09:25:58,348\tWARNING tune.py:954 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Continue running this experiment with: Tuner.restore(path=\"/home/phahn/ray_results/pretrain_2023-04-27_09-25-31\", trainable=...)\n",
      "2023-04-27 09:25:58,351\tWARNING experiment_analysis.py:644 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m 2023-04-27 09:25:58,360\tERROR worker.py:844 -- Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1197, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1100, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"python/ray/_raylet.pyx\", line 823, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"python/ray/_raylet.pyx\", line 870, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"python/ray/_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"python/ray/_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"python/ray/_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 670, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 381, in train\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 376, in step\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     result = self._results_queue.get(\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/queue.py\", line 180, in get\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/threading.py\", line 316, in wait\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m   File \"/home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_private/worker.py\", line 841, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m [2023-04-27 09:25:58,397 C 12651 12651] core_worker.cc:767:  Check failed: _s.ok() Bad status: IOError: Broken pipe\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0xd6f36a) [0x7ff2a187136a] ray::operator<<()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0xd70e52) [0x7ff2a1872e52] ray::SpdLogMessage::Flush()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray6RayLogD1Ev+0x37) [0x7ff2a1873167] ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker4ExitENS_3rpc14WorkerExitTypeERKSsRKSt10shared_ptrINS_17LocalMemoryBufferEE+0x1ab) [0x7ff2a116161b] ray::core::CoreWorker::Exit()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker11ExecuteTaskERKNS_17TaskSpecificationERKSt10shared_ptrISt13unordered_mapISsSt6vectorISt4pairIldESaIS9_EESt4hashISsESt8equal_toISsESaIS8_IKSsSB_EEEEPS7_IS8_INS_8ObjectIDES5_INS_9RayObjectEEESaISQ_EEST_PN6google8protobuf16RepeatedPtrFieldINS_3rpc20ObjectReferenceCountEEEPbPSs+0x1d10) [0x7ff2a1172270] ray::core::CoreWorker::ExecuteTask()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZNSt17_Function_handlerIFN3ray6StatusERKNS0_17TaskSpecificationESt10shared_ptrISt13unordered_mapISsSt6vectorISt4pairIldESaIS9_EESt4hashISsESt8equal_toISsESaIS8_IKSsSB_EEEEPS7_IS8_INS0_8ObjectIDES5_INS0_9RayObjectEEESaISO_EESR_PN6google8protobuf16RepeatedPtrFieldINS0_3rpc20ObjectReferenceCountEEEPbPSsESt5_BindIFMNS0_4core10CoreWorkerEFS1_S4_RKSK_SR_SR_SY_SZ_S10_EPS14_St12_PlaceholderILi1EES1A_ILi2EES1A_ILi3EES1A_ILi4EES1A_ILi5EES1A_ILi6EES1A_ILi7EEEEE9_M_invokeERKSt9_Any_dataS4_OSK_OSR_S1P_OSY_OSZ_OS10_+0x54) [0x7ff2a10b7304] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x69829e) [0x7ff2a119a29e] ray::core::CoreWorkerDirectTaskReceiver::HandleTask()::{lambda()#1}::operator()()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x6993da) [0x7ff2a119b3da] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x6abede) [0x7ff2a11adede] ray::core::InboundRequest::Accept()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x6b06d8) [0x7ff2a11b26d8] ray::core::ActorSchedulingQueue::ScheduleRequests()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core20ActorSchedulingQueue3AddEllSt8functionIFvS2_IFvNS_6StatusES2_IFvvEES5_EEEES9_S7_RKSsRKSt10shared_ptrINS_27FunctionDescriptorInterfaceEENS_6TaskIDERKSt6vectorINS_3rpc15ObjectReferenceESaISK_EE+0x596) [0x7ff2a11b4496] ray::core::ActorSchedulingQueue::Add()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core28CoreWorkerDirectTaskReceiver10HandleTaskENS_3rpc15PushTaskRequestEPNS2_13PushTaskReplyESt8functionIFvNS_6StatusES6_IFvvEES9_EE+0x1209) [0x7ff2a1199cd9] ray::core::CoreWorkerDirectTaskReceiver::HandleTask()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x64dc8a) [0x7ff2a114fc8a] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x929ae6) [0x7ff2a142bae6] EventTracker::RecordExecution()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x8c78be) [0x7ff2a13c98be] std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x8c7e16) [0x7ff2a13c9e16] boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0xd80ebb) [0x7ff2a1882ebb] boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0xd82989) [0x7ff2a1884989] boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0xd82e42) [0x7ff2a1884e42] boost::asio::io_context::run()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorker20RunTaskExecutionLoopEv+0x1c) [0x7ff2a113f27c] ray::core::CoreWorker::RunTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImpl26RunWorkerTaskExecutionLoopEv+0x8c) [0x7ff2a117cd7c] ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess20RunTaskExecutionLoopEv+0x1d) [0x7ff2a117cf2d] ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /home/phahn/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/_raylet.so(+0x4de397) [0x7ff2a0fe0397] __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0x1a1b24) [0x55afab806b24] method_vectorcall_NOARGS\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0xff2da) [0x55afab7642da] _PyEval_EvalFrameDefault.cold.2983\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(_PyFunction_Vectorcall+0x104) [0x55afab7fcbe4] _PyFunction_Vectorcall\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0xff2da) [0x55afab7642da] _PyEval_EvalFrameDefault.cold.2983\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0x196fe3) [0x55afab7fbfe3] _PyEval_EvalCode\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(PyEval_EvalCodeEx+0x4c) [0x55afab8a8a7c] PyEval_EvalCodeEx\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(PyEval_EvalCode+0x1b) [0x55afab7fcdbb] PyEval_EvalCode\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0x243b2b) [0x55afab8a8b2b] run_eval_code_obj\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0x274155) [0x55afab8d9155] run_mod\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0x1151f7) [0x55afab77a1f7] pyrun_file.cold.3078\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(PyRun_SimpleFileExFlags+0x1bf) [0x55afab8de72f] PyRun_SimpleFileExFlags\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(Py_RunMain+0x378) [0x55afab8dedf8] Py_RunMain\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(Py_BytesMain+0x39) [0x55afab8deff9] Py_BytesMain\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7) [0x7ff2a240bc87] __libc_start_main\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m ray::ImplicitFunc(+0x2016a0) [0x55afab8666a0]\n",
      "\u001b[2m\u001b[36m(pretrain pid=12651)\u001b[0m \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: unsup_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m tuner \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mTuner(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     objective,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     tune_config\u001b[39m=\u001b[39mtune_config,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     param_space\u001b[39m=\u001b[39msearch_space\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m results \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mfit()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest Pretrain-NLL Stats: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(results\u001b[39m.\u001b[39;49mget_best_result()\u001b[39m.\u001b[39mmetrics))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest Pretrain-NLL Hyperparameter: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(results\u001b[39m.\u001b[39mget_best_result(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsup_loss\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mconfig))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bworkstation/home/phahn/uncertainty-evaluation/notebooks/semi_supervised_learning/simclr_hp_search.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m opt_pretrain_config \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mget_best_result()\u001b[39m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/miniconda3/envs/torchal/lib/python3.9/site-packages/ray/tune/result_grid.py:158\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    147\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo best trial found for the given metric: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_analysis\u001b[39m.\u001b[39mdefault_metric\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis means that no trial has reported this metric\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[1;32m    152\u001b[0m     error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[39mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    156\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: unsup_loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Init ray, if we are using slurm, set cpu and gpus\n",
    "adress = None\n",
    "num_cpus = int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(address=adress, num_cpus=num_cpus, num_gpus=num_gpus, ignore_reinit_error=True)\n",
    "\n",
    "# Setup Search space\n",
    "search_space, points_to_evaluate = build_pretrain_search_space(args)\n",
    "search_alg = OptunaSearch()\n",
    "tune_config = tune.TuneConfig(search_alg=search_alg, num_samples=25,  metric='unsup_loss', mode='min')\n",
    "\n",
    "# Setup DS\n",
    "labeled_ds, unlabeled_ds, val_ds, ds_info = build_datasets(args)\n",
    "\n",
    "# Setup objective\n",
    "objective = tune.with_resources(pretrain, resources={'cpu': 1, 'gpu': 1})\n",
    "objective = tune.with_parameters(objective, args=args)\n",
    "\n",
    "# Start hyperparameter search\n",
    "tuner = tune.Tuner(\n",
    "    objective,\n",
    "    tune_config=tune_config,\n",
    "    param_space=search_space\n",
    ")\n",
    "results = tuner.fit()\n",
    "print('Best Pretrain-NLL Stats: {}'.format(results.get_best_result().metrics))\n",
    "print('Best Pretrain-NLL Hyperparameter: {}'.format(results.get_best_result(metric=\"unsup_loss\", mode=\"min\").config))\n",
    "opt_pretrain_config = results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['pretrain_learning_rate'] = float(opt_pretrain_config['lr'])\n",
    "args['pretrain_weight_decay'] = float(opt_pretrain_config['weight_decay'])\n",
    "args['noise'] = float(opt_pretrain_config['noise'])\n",
    "unlabeled_ds.noise = args['noise']\n",
    "\n",
    "seed_everything(args['random_seed'])\n",
    "model = Net(feature_dim=args['feature_dim'], projection_dim=args['projection_dim'])\n",
    "initial_model_state = copy.deepcopy(model.state_dict())\n",
    "pretrain_optimizer = torch.optim.SGD(model.parameters(), lr=args['pretrain_learning_rate'], momentum=0.9, weight_decay=args['pretrain_weight_decay'], nesterov=True)\n",
    "pretrain_criterion = NTXent(batch_size=args['pretrain_batch_size'], temperature=args['temperature'], hidden_norm=True)\n",
    "pretrain_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(pretrain_optimizer, T_max=args['pretrain_n_epochs']-args['n_epochs_warmup'])\n",
    "pretrain_loader = torch.utils.data.DataLoader(unlabeled_ds, batch_size=args['pretrain_batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "history_fe = []\n",
    "\n",
    "for i in tqdm(range(args['pretrain_n_epochs'])):\n",
    "    pretrain_stats = pretrain_one_epoch(model, pretrain_optimizer, pretrain_criterion, pretrain_loader)\n",
    "    history_fe.append({'train_stats':pretrain_stats})\n",
    "    if i > args['n_epochs_warmup']:\n",
    "        pretrain_lr_scheduler.step()\n",
    "\n",
    "pretrained_model_state = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5), constrained_layout=True)\n",
    "plt.title(\"Pretraining Statistics\")\n",
    "plt.plot([h['train_stats']['loss'] for h in history_fe])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init ray, if we are using slurm, set cpu and gpus\n",
    "adress = None\n",
    "num_cpus = int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(address=adress, num_cpus=num_cpus, num_gpus=num_gpus, ignore_reinit_error=True)\n",
    "\n",
    "# Setup Search space\n",
    "search_space, points_to_evaluate = build_supervised_search_space(args)\n",
    "search_alg = OptunaSearch()\n",
    "tune_config = tune.TuneConfig(search_alg=search_alg, num_samples=25,  metric='val_acc', mode='max')\n",
    "\n",
    "# Setup objective\n",
    "objective = tune.with_resources(train, resources={'cpu': 1, 'gpu': 1})\n",
    "model = Net(feature_dim=args['feature_dim'], projection_dim=args['projection_dim'])\n",
    "objective = tune.with_parameters(objective, args=args, labeled_ds=labeled_ds, val_ds=val_ds, pretrained_model_state=pretrained_model_state, model=model)\n",
    "\n",
    "# Start hyperparameter search\n",
    "tuner = tune.Tuner(\n",
    "    objective,\n",
    "    tune_config=tune_config,\n",
    "    param_space=search_space\n",
    ")\n",
    "results = tuner.fit()\n",
    "print('Best Supervised Stats: {}'.format(results.get_best_result().metrics))\n",
    "print('Best Validation Acc Hyperparameter: {}'.format(results.get_best_result(metric=\"val_acc\", mode=\"max\").config))\n",
    "opt_supervised_config = results.get_best_result().config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['supervised_learning_rate'] = opt_supervised_config['lr']\n",
    "args['supervised_weight_decay'] = opt_supervised_config['weight_decay']\n",
    "args['supervised_n_epochs'] = opt_supervised_config['n_epochs']\n",
    "\n",
    "model.load_state_dict(pretrained_model_state)\n",
    "\n",
    "# freeze all layers but the last fc\n",
    "for name, param in model.named_parameters():\n",
    "    if name not in ['fc.weight', 'fc.bias']:\n",
    "        param.requires_grad = False\n",
    "\n",
    "supervised_optimizer = torch.optim.SGD(model.parameters(), lr=args['supervised_learning_rate'], momentum=0.9, weight_decay=args['supervised_weight_decay'], nesterov=True)\n",
    "supervised_lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(supervised_optimizer, T_max=args['supervised_n_epochs'])\n",
    "supervised_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(labeled_ds, batch_size=args['supervised_batch_size'], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=64)\n",
    "\n",
    "history_fc = []\n",
    "\n",
    "for j in range(args['supervised_n_epochs']):\n",
    "    train_stats = train_one_epoch(model, supervised_optimizer, supervised_criterion, train_loader)\n",
    "    test_stats = evaluate(model, supervised_criterion, val_loader)\n",
    "    supervised_lr_scheduler.step()\n",
    "    history_fc.append(\n",
    "        {\n",
    "        'train_stats': train_stats,\n",
    "        'test_stats':test_stats\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(10,8), constrained_layout=True)\n",
    "fig.suptitle('Training Statistics for SimCLR')\n",
    "ax[0][0].plot([h['train_stats']['acc']*100 for h in history_fc])\n",
    "ax[0][0].set_xlabel('epoch')\n",
    "ax[0][0].set_ylabel('Train Accuracy(%)')\n",
    "ax[0][1].plot([h['train_stats']['loss'] for h in history_fc])\n",
    "ax[0][1].set_xlabel('epoch')\n",
    "ax[0][1].set_ylabel('Train Loss (Avg.)')\n",
    "\n",
    "ax[1][0].plot([h['test_stats']['acc']*100 for h in history_fc])\n",
    "ax[1][0].set_xlabel('epoch')\n",
    "ax[1][0].set_ylabel('Test Accuracy(%)')\n",
    "ax[1][1].plot([h['test_stats']['loss'] for h in history_fc])\n",
    "ax[1][1].set_xlabel('epoch')\n",
    "ax[1][1].set_ylabel('Test Loss (Avg.)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison to basic NN Fully supervised training\n",
    "n_epochs = 100\n",
    "batch_size = 8\n",
    "learning_rate = 1e-2\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "\n",
    "baseline = Net(num_classes=2, feature_dim=args[\"feature_dim\"], projection_dim=args[\"projection_dim\"])\n",
    "baseline.load_state_dict(initial_model_state)\n",
    "optimizer = torch.optim.SGD(baseline.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "dataloader = torch.utils.data.DataLoader(labeled_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(val_ds, batch_size=64)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train_stats = train_one_epoch(model=baseline, supervised_loader=dataloader, criterion=criterion, optimizer=optimizer, device='cuda')\n",
    "    test_stats = evaluate(model=baseline, test_loader=dataloader_val, criterion=criterion, device='cuda')\n",
    "    history.append(\n",
    "        {\n",
    "        'train_stats': train_stats,\n",
    "        'test_stats':test_stats\n",
    "        }\n",
    "    )\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(10,8), constrained_layout=True)\n",
    "fig.suptitle('Training Statistics for Baseline')\n",
    "ax[0][0].plot([h['train_stats']['acc'] for h in history])\n",
    "ax[0][0].set_xlabel('epoch')\n",
    "ax[0][0].set_ylabel('Train Accuracy(%)')\n",
    "ax[0][1].plot([h['train_stats']['loss'] for h in history])\n",
    "ax[0][1].set_xlabel('epoch')\n",
    "ax[0][1].set_ylabel('Train Loss (Avg.)')\n",
    "\n",
    "ax[1][0].plot([h['test_stats']['acc'] for h in history])\n",
    "ax[1][0].set_xlabel('epoch')\n",
    "ax[1][0].set_ylabel('Test Accuracy(%)')\n",
    "ax[1][1].plot([h['test_stats']['loss'] for h in history])\n",
    "ax[1][1].set_xlabel('epoch')\n",
    "ax[1][1].set_ylabel('Test Loss (Avg.)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, labeled_indices, unlabeled_indices = ds_info['X'], ds_info['y'], ds_info['labeled_indices'], ds_info['unlabeled_indices']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5), constrained_layout=True)\n",
    "fig.suptitle(\"Comparison of Decision Boundary\")\n",
    "plot_contour(baseline, X[labeled_indices], y[labeled_indices], X[unlabeled_indices], y[unlabeled_indices], ax[0])\n",
    "ax[0].set_title(\"Baseline\")\n",
    "plot_contour(model, X[labeled_indices], y[labeled_indices], X[unlabeled_indices], y[unlabeled_indices], ax[1])\n",
    "ax[1].set_title(\"Pretrained (SimCLR)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
